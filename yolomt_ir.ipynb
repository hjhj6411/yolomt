{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1nVvwpOwRWuSG2C1dea5o-CqMEcBAJgpo",
      "authorship_tag": "ABX9TyPwFvV8IkWFCdfq9LnR5YL3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hjhj6411/yolomt/blob/main/yolomt_ir.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ê¸°ì¡´ ì„¤ì¹˜ ì‹œë„ ì°Œêº¼ê¸° ì œê±° (í˜¹ì‹œ ëª¨ë¥¼ ì¶©ëŒ ë°©ì§€)\n",
        "!pip uninstall -y onnx onnxsim protobuf\n",
        "\n",
        "# 2. Python 3.12ì™€ í˜¸í™˜ë˜ëŠ” ì•ˆì •ì ì¸ ë²„ì „ ì„¤ì¹˜\n",
        "# onnx>=1.16.0 : Python 3.12 ì§€ì›\n",
        "# onnxsim>=0.4.36 : ìµœì‹  ONNXì™€ í˜¸í™˜\n",
        "!pip install \"onnx>=1.16.0\" \"onnxsim>=0.4.36\"\n",
        "\n",
        "print(\"âœ“ ONNX & ONNX-Simplifier installed successfully\")"
      ],
      "metadata": {
        "id": "M1NbaJrFxNfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. PyTorch 2.2.0 + CUDA 11.8 ì„¤ì¹˜\n",
        "# (Linux/Colab ê¸°ì¤€)\n",
        "!pip install torch==2.2.0+cu118 torchvision==0.17.0+cu118 torchaudio==2.2.0+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# 2. NumPy 1.26.4 ì„¤ì¹˜ (ë§¤ìš° ì¤‘ìš”)\n",
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "bLh8DFLjygzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zuhqs-AQxEe6",
        "outputId": "51a0a0c9-367d-4b52-a06e-8f73450dbb24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "YOLOMT Training Environment Check\n",
            "======================================================================\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "PyTorch version: 2.2.0+cu118\n",
            "CUDA available: True\n",
            "CUDA version: 11.8\n",
            "GPU: Tesla T4\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"YOLOMT Training Environment Check\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"ğŸš€ YOLOMT with 300W-LP Dataset - Complete Pipeline\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import onnx\n",
        "from onnxsim import simplify\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nâœ“ Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "bW2fDPTCxFiA",
        "outputId": "51b0b46b-5f72-4ba8-b085-e71b3e70ff7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ğŸš€ YOLOMT with 300W-LP Dataset - Complete Pipeline\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mInstalling onnxruntime by `/usr/bin/python3 -m pip install onnxruntime`, please wait for a moment..\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Installing onnxruntime by `/usr/bin/python3 -m pip install onnxruntime`, please wait for a moment..</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Device: cuda\n",
            "âœ“ GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/archive.zip\" \"/content/temp_dataset.zip\"\n",
        "\n",
        "!unzip -qq \"/content/temp_dataset.zip\" -d \"/content/dataset\"\n",
        "\n",
        "print(\"ì••ì¶• í•´ì œ ì™„ë£Œ!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW2iRIhWxMH9",
        "outputId": "a5a2fb36-fab3-4209-d00c-68978587ae21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì••ì¶• í•´ì œ ì™„ë£Œ!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_ROOT = '/content/dataset/300W_LP'\n",
        "\n",
        "USE_DUMMY_DATA = not os.path.exists(DATA_ROOT)\n",
        "\n",
        "if USE_DUMMY_DATA:\n",
        "    print(\"\\nâš ï¸  300W-LP not found, using dummy data for demonstration\")\n",
        "    print(\"   (ì„±ëŠ¥ì€ ë‚˜ì˜¤ì§€ ì•Šì§€ë§Œ ì „ì²´ íŒŒì´í”„ë¼ì¸ì€ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥)\")\n"
      ],
      "metadata": {
        "id": "9cSGRxmuxylF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RepConv(nn.Module):\n",
        "    def __init__(self, c1, c2, k=3, s=1, p=None, g=1, act=True):\n",
        "        super().__init__()\n",
        "        self.deploy = False\n",
        "        self.act = nn.SiLU() if act else nn.Identity()\n",
        "        self.g = g  # ê·¸ë£¹ ì •ë³´ ì €ì¥ (ë§¤ìš° ì¤‘ìš”)\n",
        "\n",
        "        if p is None: p = k // 2\n",
        "\n",
        "        self.conv_kxk = nn.Sequential(\n",
        "            nn.Conv2d(c1, c2, k, s, p, groups=g, bias=False),\n",
        "            nn.BatchNorm2d(c2)\n",
        "        )\n",
        "\n",
        "        self.conv_1x1 = nn.Sequential(\n",
        "            nn.Conv2d(c1, c2, 1, s, 0, groups=g, bias=False),\n",
        "            nn.BatchNorm2d(c2)\n",
        "        ) if k > 1 else None\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(c1) if c2 == c1 and s == 1 else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.deploy:\n",
        "            return self.act(self.conv_fused(x))\n",
        "\n",
        "        y = self.conv_kxk(x)\n",
        "        if self.conv_1x1:\n",
        "            y += self.conv_1x1(x)\n",
        "        if self.bn:\n",
        "            y += self.bn(x)\n",
        "        return self.act(y)\n",
        "\n",
        "    def fuse(self):\n",
        "        if self.deploy: return\n",
        "        kernel, bias = self._get_equivalent_kernel_bias()\n",
        "\n",
        "        # Fused Conv ìƒì„± (groups ì„¤ì • ìœ ì§€)\n",
        "        self.conv_fused = nn.Conv2d(\n",
        "            self.conv_kxk[0].in_channels,\n",
        "            self.conv_kxk[0].out_channels,\n",
        "            self.conv_kxk[0].kernel_size,\n",
        "            self.conv_kxk[0].stride,\n",
        "            self.conv_kxk[0].padding,\n",
        "            groups=self.conv_kxk[0].groups, # groups ê°’ ì „ë‹¬\n",
        "            bias=True\n",
        "        )\n",
        "        self.conv_fused.weight.data = kernel\n",
        "        self.conv_fused.bias.data = bias\n",
        "\n",
        "        # ê°€ì§€ì¹˜ê¸°\n",
        "        for attr in ['conv_kxk', 'conv_1x1', 'bn']:\n",
        "            if hasattr(self, attr): delattr(self, attr)\n",
        "        self.deploy = True\n",
        "\n",
        "    def _get_equivalent_kernel_bias(self):\n",
        "        k3, b3 = self._fuse_bn(self.conv_kxk[0], self.conv_kxk[1])\n",
        "        k1, b1 = 0, 0\n",
        "        if self.conv_1x1:\n",
        "            k1, b1 = self._fuse_bn(self.conv_1x1[0], self.conv_1x1[1])\n",
        "            pad = (self.conv_kxk[0].kernel_size[0] - 1) // 2\n",
        "            k1 = F.pad(k1, [pad]*4)\n",
        "        kb, bb = 0, 0\n",
        "        if self.bn:\n",
        "            kb, bb = self._fuse_bn_identity(self.bn)\n",
        "        return k3 + k1 + kb, b3 + b1 + bb\n",
        "\n",
        "    @staticmethod\n",
        "    def _fuse_bn(conv, bn):\n",
        "        std = (bn.running_var + bn.eps).sqrt()\n",
        "        t = (bn.weight / std).reshape(-1, 1, 1, 1)\n",
        "        return conv.weight * t, bn.bias - bn.running_mean * bn.weight / std\n",
        "\n",
        "    def _fuse_bn_identity(self, bn):\n",
        "        # ğŸ”§ [ìˆ˜ì •ëœ ë¶€ë¶„] ê·¸ë£¹ ì—°ì‚°(Depthwise)ì¼ ê²½ìš° ì°¨ì›ì„ ë§ì¶°ì£¼ëŠ” ë¡œì§ ì¶”ê°€\n",
        "        k_size = self.conv_kxk[0].kernel_size[0]\n",
        "        in_ch = bn.num_features\n",
        "\n",
        "        # ì¤‘ìš”: self.g > 1ì´ë©´ (C, 1, K, K) í˜•íƒœì—¬ì•¼ í•¨\n",
        "        if self.g > 1:\n",
        "             kernel = torch.zeros((in_ch, 1, k_size, k_size), device=bn.weight.device)\n",
        "             for i in range(in_ch):\n",
        "                 kernel[i, 0, k_size//2, k_size//2] = 1\n",
        "        else:\n",
        "             kernel = torch.zeros((in_ch, in_ch, k_size, k_size), device=bn.weight.device)\n",
        "             for i in range(in_ch):\n",
        "                 kernel[i, i, k_size//2, k_size//2] = 1\n",
        "\n",
        "        std = (bn.running_var + bn.eps).sqrt()\n",
        "        t = (bn.weight / std).reshape(-1, 1, 1, 1)\n",
        "        return kernel * t, bn.bias - bn.running_mean * bn.weight / std\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2. ë‚˜ë¨¸ì§€ í´ë˜ìŠ¤ (RepBottleneck, RepC2f, YOLOMT)\n",
        "# ==========================================\n",
        "# ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•˜ì§€ë§Œ, RepConvê°€ ìˆ˜ì •ë˜ì—ˆìœ¼ë¯€ë¡œ ë‹¤ì‹œ ì •ì˜í•´ì•¼ ì ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "class RepBottleneck(nn.Module):\n",
        "    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):\n",
        "        super().__init__()\n",
        "        c_ = int(c2 * e)\n",
        "        self.cv1 = RepConv(c1, c_, k[0], 1)\n",
        "        self.cv2 = RepConv(c_, c2, k[1], 1, g=c_)\n",
        "        self.add = shortcut and c1 == c2\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
        "\n",
        "class RepC2f(nn.Module):\n",
        "    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):\n",
        "        super().__init__()\n",
        "        self.c = int(c2 * e)\n",
        "        # ì—¬ê¸°ì„œ k=1ì¼ ë•Œ RepConvê°€ ì•Œì•„ì„œ p=0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì—ëŸ¬ í•´ê²°ë¨\n",
        "        self.cv1 = RepConv(c1, 2 * self.c, 1, 1)\n",
        "        self.cv2 = RepConv((2 + n) * self.c, c2, 1)\n",
        "        self.m = nn.ModuleList(RepBottleneck(self.c, self.c, shortcut, g, k=(3, 3), e=1.0) for _ in range(n))\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = list(self.cv1(x).chunk(2, 1))\n",
        "        y.extend(m(y[-1]) for m in self.m)\n",
        "        return self.cv2(torch.cat(y, 1))\n",
        "\n",
        "class SimplifiedYOLOMT(nn.Module):\n",
        "    def __init__(self, nc=1, n_keypoints=68):\n",
        "        super().__init__()\n",
        "        self.nc = nc\n",
        "        self.stem = nn.Sequential(RepConv(3, 32, 3, 2), RepConv(32, 64, 3, 2))\n",
        "        self.stage1 = RepC2f(64, 64, n=1, shortcut=True)\n",
        "        self.down1 = RepConv(64, 128, 3, 2)\n",
        "        self.stage2 = RepC2f(128, 128, n=2, shortcut=True)\n",
        "        self.down2 = RepConv(128, 256, 3, 2)\n",
        "        self.stage3 = RepC2f(256, 256, n=2, shortcut=True)\n",
        "        self.det_head = nn.Sequential(RepConv(256, 128, 3, 1), nn.Conv2d(128, 5 + nc, 1))\n",
        "        self.kpt_head = nn.Sequential(RepConv(256, 128, 3, 1), nn.Conv2d(128, n_keypoints * 3, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.stage1(x)\n",
        "        x = self.down1(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.down2(x)\n",
        "        x = self.stage3(x)\n",
        "        return self.det_head(x), self.kpt_head(x)\n",
        "\n",
        "    def fuse(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, RepConv): m.fuse()\n",
        "        return self\n",
        "\n",
        "print(\"âœ“ Model fixed: AutoPad applied to RepConv\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 3. YOLOMT Model (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„± ìœ ì§€)\n",
        "# ==========================================\n",
        "\n",
        "class SimplifiedYOLOMT(nn.Module):\n",
        "    \"\"\"\n",
        "    YOLOMT Architecture with RepC2f Blocks\n",
        "    ì…ì¶œë ¥ êµ¬ì¡°ëŠ” ê¸°ì¡´ Training Loopì™€ í˜¸í™˜ë˜ë„ë¡ ìœ ì§€í•¨.\n",
        "    \"\"\"\n",
        "    def __init__(self, nc=1, n_keypoints=68):\n",
        "        super().__init__()\n",
        "        self.nc = nc\n",
        "\n",
        "        # Stem: Low-level features [cite: 222]\n",
        "        self.stem = nn.Sequential(\n",
        "            RepConv(3, 32, 3, 2, 1), # Stride 2\n",
        "            RepConv(32, 64, 3, 2, 1) # Stride 2\n",
        "        )\n",
        "\n",
        "        # Backbone Stage 1 (P3)\n",
        "        # ê¸°ì¡´ Sequential ëŒ€ì‹  RepC2f ì‚¬ìš©ìœ¼ë¡œ ê¹Šì€ íŠ¹ì§• ì¶”ì¶œ\n",
        "        self.stage1 = RepC2f(64, 64, n=1, shortcut=True)\n",
        "        self.down1 = RepConv(64, 128, 3, 2, 1) # Downsample\n",
        "\n",
        "        # Backbone Stage 2 (P4)\n",
        "        self.stage2 = RepC2f(128, 128, n=2, shortcut=True)\n",
        "        self.down2 = RepConv(128, 256, 3, 2, 1) # Downsample\n",
        "\n",
        "        # Backbone Stage 3 (P5)\n",
        "        self.stage3 = RepC2f(256, 256, n=2, shortcut=True)\n",
        "\n",
        "        # Detection Head (Decoupled Head [cite: 284])\n",
        "        # ê¸°ì¡´ Training Loopì˜ ë‹¨ìˆœí•¨ì— ë§ì¶° ì¶œë ¥ ì±„ë„ë§Œ ì¡°ì •\n",
        "        self.det_head = nn.Sequential(\n",
        "            RepConv(256, 128, 3, 1),\n",
        "            nn.Conv2d(128, 5 + nc, 1) # Box(4) + Conf(1) + Class(nc)\n",
        "        )\n",
        "\n",
        "        # Keypoint Head (Decoupled Head)\n",
        "        self.kpt_head = nn.Sequential(\n",
        "            RepConv(256, 128, 3, 1),\n",
        "            nn.Conv2d(128, n_keypoints * 3, 1) # (x, y, vis) * 68 [cite: 264]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feed-forward (Single Path)\n",
        "        x = self.stem(x)      # Stem\n",
        "        x = self.stage1(x)    # P3\n",
        "        x = self.down1(x)     # Down\n",
        "        x = self.stage2(x)    # P4\n",
        "        x = self.down2(x)     # Down\n",
        "        x = self.stage3(x)    # P5 (Deepest features)\n",
        "\n",
        "        # Head\n",
        "        det = self.det_head(x)\n",
        "        kpt = self.kpt_head(x)\n",
        "\n",
        "        return det, kpt\n",
        "\n",
        "    def fuse(self):\n",
        "        \"\"\"Reparameterization for Inference\"\"\"\n",
        "        print(\"Fusing model layers for inference...\")\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, RepConv):\n",
        "                m.fuse()\n",
        "        return self\n",
        "\n",
        "print(\"âœ“ YOLOMT Model updated with RepC2f & Bottleneck (Ready for Training)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLrgxaAyxzDZ",
        "outputId": "21006796-caa4-4557-d009-45cd5069d90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Model fixed: AutoPad applied to RepConv\n",
            "âœ“ YOLOMT Model updated with RepC2f & Bottleneck (Ready for Training)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PART 3: Dataset Implementation (ìˆ˜ì •ë²„ì „)\n",
        "# ==========================================\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import scipy.io\n",
        "\n",
        "class FaceLandmarkDataset(Dataset):\n",
        "    def __init__(self, data_root, img_size=320, augment=False):\n",
        "        self.data_root = data_root\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "\n",
        "        self.images = []\n",
        "\n",
        "        # ë°ì´í„° ê²½ë¡œ í™•ì¸ ë° ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
        "        if os.path.exists(data_root):\n",
        "            # 300W-LP í´ë” êµ¬ì¡°ì— ë§ì¶° íƒìƒ‰\n",
        "            # (ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  í•˜ìœ„ í´ë”ì˜ jpgë¥¼ ì°¾ìŠµë‹ˆë‹¤)\n",
        "            self.images = glob.glob(os.path.join(data_root, '**', '*.jpg'), recursive=True)\n",
        "\n",
        "            # ë”ë¯¸/ë°±ì—… íŒŒì¼ ë“±ì´ ì„ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì •ë ¬\n",
        "            self.images = sorted(self.images)\n",
        "\n",
        "            print(f\"âœ“ Found {len(self.images)} images in {data_root}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"âŒ ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤: {data_root}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 1. ì´ë¯¸ì§€ ë¡œë“œ\n",
        "        img_path = self.images[idx]\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ ì²´í¬ (ë§¤ìš° ì¤‘ìš”!)\n",
        "        if img is None:\n",
        "            raise ValueError(f\"âŒ ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {img_path}\")\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # 2. ë¼ë²¨(.mat) ë¡œë“œ\n",
        "        mat_path = img_path.replace('.jpg', '.mat')\n",
        "\n",
        "        if not os.path.exists(mat_path):\n",
        "            raise FileNotFoundError(f\"âŒ ë¼ë²¨ íŒŒì¼(.mat)ì´ ì—†ìŠµë‹ˆë‹¤: {mat_path}\")\n",
        "\n",
        "        try:\n",
        "            mat_data = scipy.io.loadmat(mat_path)\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"âŒ .mat íŒŒì¼ ì†ìƒë¨: {mat_path} / ì—ëŸ¬: {e}\")\n",
        "\n",
        "        # 3. ë°ì´í„° íŒŒì‹±\n",
        "        try:\n",
        "            pt2d = mat_data['pt2d'] # (2, 68)\n",
        "            landmarks = torch.from_numpy(pt2d.T).float() # (68, 2)\n",
        "\n",
        "            # Bounding box ê³„ì‚°\n",
        "            x_min, y_min = landmarks.min(dim=0)[0]\n",
        "            x_max, y_max = landmarks.max(dim=0)[0]\n",
        "            w, h = x_max - x_min, y_max - y_min\n",
        "            cx, cy = (x_min + x_max) / 2, (y_min + y_max) / 2\n",
        "\n",
        "            # ì •ê·œí™” (0-1)\n",
        "            img_h, img_w = img.shape[:2]\n",
        "\n",
        "            # âš ï¸ Zero Division ë°©ì§€\n",
        "            if img_w == 0 or img_h == 0:\n",
        "                 raise ValueError(f\"ì´ë¯¸ì§€ í¬ê¸°ê°€ 0ì…ë‹ˆë‹¤: {img_path}\")\n",
        "\n",
        "            bbox = torch.tensor([[cx/img_w, cy/img_h, w/img_w, h/img_h]])\n",
        "            landmarks[:, 0] /= img_w\n",
        "            landmarks[:, 1] /= img_h\n",
        "\n",
        "            # ê°’ ë²”ìœ„ ì²´í¬ (0~1 ë²—ì–´ë‚˜ëŠ”ì§€ í™•ì¸)\n",
        "            # 300W-LPëŠ” ê°€ë” ëœë“œë§ˆí¬ê°€ ì´ë¯¸ì§€ ë°–ìœ¼ë¡œ ë‚˜ê°€ëŠ” ê²½ìš°ê°€ ìˆìŒ -> í´ë¨í•‘\n",
        "            landmarks = torch.clamp(landmarks, 0.0, 1.0)\n",
        "\n",
        "            target = {\n",
        "                'bbox': bbox,\n",
        "                'landmarks': landmarks,\n",
        "                'visibility': torch.ones(68)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"âŒ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ìˆ˜ì‹ ì—ëŸ¬ ë°œìƒ ({img_path}): {e}\")\n",
        "\n",
        "        # 4. ë¦¬ì‚¬ì´ì¦ˆ (Letterbox)\n",
        "        img_resized = self.letterbox_resize(img, self.img_size)\n",
        "        img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        return img_tensor, target\n",
        "\n",
        "    def letterbox_resize(self, img, new_size):\n",
        "        h, w = img.shape[:2]\n",
        "        if h == 0 or w == 0: return img\n",
        "\n",
        "        scale = min(new_size / h, new_size / w)\n",
        "        new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "        img_resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        pad_h = (new_size - new_h) // 2\n",
        "        pad_w = (new_size - new_w) // 2\n",
        "\n",
        "        img_padded = np.full((new_size, new_size, 3), 114, dtype=np.uint8)\n",
        "        img_padded[pad_h:pad_h+new_h, pad_w:pad_w+new_w] = img_resized\n",
        "\n",
        "        return img_padded\n",
        "\n",
        "print(\"âœ… ìˆ˜ì •ëœ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ (ì—ëŸ¬ ê²€ì¶œ ëª¨ë“œ)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M14dn0GQx4F1",
        "outputId": "598aee5e-2c9c-4dcf-fd48-7db413e7b1fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ìˆ˜ì •ëœ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ (ì—ëŸ¬ ê²€ì¶œ ëª¨ë“œ)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PART 4: Training Setup\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 16\n",
        "IMG_SIZE = 320\n",
        "LR = 0.01\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "print(f\"Hyperparameters:\")\n",
        "print(f\"  - Epochs: {EPOCHS}\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  - Image size: {IMG_SIZE}\")\n",
        "print(f\"  - Learning rate: {LR}\")\n",
        "\n",
        "# Dataset & DataLoader\n",
        "train_dataset = FaceLandmarkDataset(\n",
        "    data_root=DATA_ROOT,\n",
        "    img_size=IMG_SIZE,\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Dataset: {len(train_dataset)} samples\")\n",
        "print(f\"âœ“ DataLoader: {len(train_loader)} batches\")\n",
        "\n",
        "# ëª¨ë¸\n",
        "model = SimplifiedYOLOMT(nc=1, n_keypoints=68).to(device)\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"âœ“ Model: {n_params/1e6:.2f}M parameters\")\n",
        "\n",
        "# Optimizer & Scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=0.0001)\n",
        "\n",
        "print(\"âœ“ Optimizer: SGD\")\n",
        "print(\"âœ“ Scheduler: CosineAnnealingLR\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BpIjgZryApS",
        "outputId": "ffd6450a-6822-467f-ed8b-2ea2130976a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "PART 4: Training Setup\n",
            "======================================================================\n",
            "Hyperparameters:\n",
            "  - Epochs: 5\n",
            "  - Batch size: 16\n",
            "  - Image size: 320\n",
            "  - Learning rate: 0.01\n",
            "âœ“ Found 122450 images in /content/dataset/300W_LP\n",
            "\n",
            "âœ“ Dataset: 122450 samples\n",
            "âœ“ DataLoader: 7654 batches\n",
            "âœ“ Model: 1.80M parameters\n",
            "âœ“ Optimizer: SGD\n",
            "âœ“ Scheduler: CosineAnnealingLR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ==========================================\n",
        "# 1. WingLoss (TypeError í•´ê²° + ìˆ˜ì‹ ë³´ì •)\n",
        "# ==========================================\n",
        "class WingLoss(nn.Module):\n",
        "    def __init__(self, w=10.0, epsilon=2.0):\n",
        "        super(WingLoss, self).__init__()\n",
        "        self.w = w\n",
        "        self.epsilon = epsilon\n",
        "        # ê³µì‹: C = w - w * ln(1 + w/epsilon)\n",
        "        self.C = self.w - self.w * math.log(1 + self.w / self.epsilon)\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        diff = torch.abs(pred - target)\n",
        "\n",
        "        # Case 1: ì˜¤ì°¨ê°€ ì‘ì„ ë•Œ (diff < w) -> Log í•¨ìˆ˜ë¡œ ë¯¼ê°í•˜ê²Œ ë°˜ì‘\n",
        "        # Case 2: ì˜¤ì°¨ê°€ í´ ë•Œ (diff >= w) -> Linear í•¨ìˆ˜ (diff - C)\n",
        "        loss = torch.where(diff < self.w,\n",
        "                           self.w * torch.log(1 + diff / self.epsilon),\n",
        "                           diff - self.C)\n",
        "        return torch.mean(loss)\n",
        "\n",
        "# ==========================================\n",
        "# 2. YOLOMTLoss (ì´ì „ê³¼ ë™ì¼, WingLoss ì—°ê²°)\n",
        "# ==========================================\n",
        "class YOLOMTLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "        self.mse = nn.MSELoss(reduction='none')\n",
        "        self.wing = WingLoss()  # ìœ„ì—ì„œ ìˆ˜ì •í•œ WingLoss ì‚¬ìš©\n",
        "\n",
        "        self.lambda_obj = 1.0\n",
        "        self.lambda_box = 5.0\n",
        "        self.lambda_kpt = 1.0\n",
        "\n",
        "    def make_grid_targets(self, pred_det, targets):\n",
        "        B, C, H, W = pred_det.shape\n",
        "        device = pred_det.device\n",
        "\n",
        "        target_obj = torch.zeros((B, 1, H, W), device=device)\n",
        "        target_box = torch.zeros((B, 4, H, W), device=device)\n",
        "        target_kpt = torch.zeros((B, 136, H, W), device=device)\n",
        "\n",
        "        mask_indices = []\n",
        "        gt_bboxes = targets['bbox']\n",
        "        gt_kpts = targets['landmarks']\n",
        "\n",
        "        for b in range(B):\n",
        "            box = gt_bboxes[b]\n",
        "            kpt = gt_kpts[b]\n",
        "\n",
        "            gx = int(box[0] * W)\n",
        "            gy = int(box[1] * H)\n",
        "            gx = max(0, min(gx, W - 1))\n",
        "            gy = max(0, min(gy, H - 1))\n",
        "\n",
        "            target_obj[b, 0, gy, gx] = 1.0\n",
        "            target_box[b, :, gy, gx] = box\n",
        "            target_kpt[b, :, gy, gx] = kpt.view(-1)\n",
        "            mask_indices.append((b, gy, gx))\n",
        "\n",
        "        return target_obj, target_box, target_kpt, mask_indices\n",
        "\n",
        "    def forward(self, pred_det, pred_kpt, targets):\n",
        "        target_obj, target_box, target_kpt, mask_indices = self.make_grid_targets(pred_det, targets)\n",
        "\n",
        "        pred_conf = pred_det[:, 4:5, :, :]\n",
        "        loss_obj = self.bce(pred_conf, target_obj).mean()\n",
        "\n",
        "        if len(mask_indices) > 0:\n",
        "            pos_mask = target_obj.bool()\n",
        "\n",
        "            # --- Box Loss ---\n",
        "            pred_box_vec = pred_det[:, :4, :, :].permute(0, 2, 3, 1)[pos_mask.squeeze(1)]\n",
        "            target_box_vec = target_box.permute(0, 2, 3, 1)[pos_mask.squeeze(1)]\n",
        "            loss_box = self.mse(torch.sigmoid(pred_box_vec), target_box_vec).mean()\n",
        "\n",
        "            # --- Keypoint Loss ---\n",
        "            pred_kpt_vec = pred_kpt.permute(0, 2, 3, 1)[pos_mask.squeeze(1)]\n",
        "            pred_kpt_vec = torch.sigmoid(pred_kpt_vec)\n",
        "\n",
        "            # (N, 204) -> (N, 136) ë³€í™˜ (x,y ì¢Œí‘œë§Œ ì¶”ì¶œ)\n",
        "            pred_kpt_vec = pred_kpt_vec.view(-1, 68, 3)[:, :, :2].reshape(-1, 136)\n",
        "\n",
        "            target_kpt_vec = target_kpt.permute(0, 2, 3, 1)[pos_mask.squeeze(1)]\n",
        "            loss_kpt = self.wing(pred_kpt_vec, target_kpt_vec)\n",
        "\n",
        "        else:\n",
        "            loss_box = torch.tensor(0.0, device=pred_det.device)\n",
        "            loss_kpt = torch.tensor(0.0, device=pred_det.device)\n",
        "\n",
        "        total_loss = (self.lambda_obj * loss_obj) + \\\n",
        "                     (self.lambda_box * loss_box) + \\\n",
        "                     (self.lambda_kpt * loss_kpt)\n",
        "\n",
        "        return total_loss, (loss_obj.item(), loss_box.item(), loss_kpt.item())"
      ],
      "metadata": {
        "id": "irtMYs40yB93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PART 5: Training (YOLO Loss ì ìš© ë²„ì „)\n",
        "# ==========================================\n",
        "\n",
        "# 1. Loss êµì²´\n",
        "criterion = YOLOMTLoss().to(device)\n",
        "\n",
        "# 2. Optimizer ì´ˆê¸°í™” (ìƒˆ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¡œ ë‹¤ì‹œ ì—°ê²° í•„ìˆ˜!)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.937, weight_decay=0.0005)\n",
        "\n",
        "model.train()\n",
        "\n",
        "print(\"ğŸš€ YOLO Training Started...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
        "\n",
        "    for batch_idx, (images, targets) in enumerate(pbar):\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Targets ë”•ì…”ë„ˆë¦¬ ê°’ë“¤ë„ GPUë¡œ ì´ë™\n",
        "        targets['bbox'] = targets['bbox'].to(device).float().squeeze(1) # [B, 1, 4] -> [B, 4] ì£¼ì˜!\n",
        "        targets['landmarks'] = targets['landmarks'].to(device).float()\n",
        "\n",
        "        # Forward (ëª¨ë¸ êµ¬ì¡° ìˆ˜ì • í›„ det, kptëŠ” [B, C, H, W] í˜•íƒœì„)\n",
        "        det_out, kpt_out = model(images)\n",
        "\n",
        "        # Loss ê³„ì‚° (ì´ì œ ì°¨ì›ì„ ë§ì¶œ í•„ìš” ì—†ì´ ë°”ë¡œ ë„£ìŠµë‹ˆë‹¤)\n",
        "        loss, loss_items = criterion(det_out, kpt_out, targets)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'Total': f'{loss.item():.4f}',\n",
        "            'Obj': f'{loss_items[0]:.4f}',\n",
        "            'Box': f'{loss_items[1]:.4f}',\n",
        "            'Kpt': f'{loss_items[2]:.4f}'\n",
        "        })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B9OsFV2yDyx",
        "outputId": "38333db4-9941-4f12-b005-0b6b35ea31b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ YOLO Training Started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7654/7654 [12:26<00:00, 10.25it/s, Total=0.0685, Obj=0.0053, Box=0.0001, Kpt=0.0627]\n",
            "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7654/7654 [12:26<00:00, 10.25it/s, Total=0.0654, Obj=0.0065, Box=0.0001, Kpt=0.0587]\n",
            "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7654/7654 [12:21<00:00, 10.32it/s, Total=0.0516, Obj=0.0039, Box=0.0000, Kpt=0.0476]\n",
            "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7654/7654 [12:29<00:00, 10.22it/s, Total=0.0494, Obj=0.0037, Box=0.0001, Kpt=0.0454]\n",
            "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7654/7654 [12:30<00:00, 10.20it/s, Total=0.0509, Obj=0.0013, Box=0.0001, Kpt=0.0490]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_prediction(model, dataset, idx=0, conf_thresh=0.5):\n",
        "    model.eval()\n",
        "\n",
        "    # --- ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ---\n",
        "    img_tensor, target = dataset[idx]\n",
        "\n",
        "    # ì´ë¯¸ì§€ ë³€í™˜\n",
        "    img_np = img_tensor.permute(1, 2, 0).cpu().numpy().copy()\n",
        "    img_np = (img_np * 255).astype(np.uint8)\n",
        "    img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "    h, w = img_np.shape[:2]\n",
        "\n",
        "    # --- ëª¨ë¸ ì¶”ë¡  ---\n",
        "    input_tensor = img_tensor.unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        det_out, kpt_out = model(input_tensor)\n",
        "\n",
        "    # --- ì˜ˆì¸¡ê°’ ë””ì½”ë”© ---\n",
        "    pred_conf = torch.sigmoid(det_out[0, 4, :, :])\n",
        "    gy, gx = np.unravel_index(torch.argmax(pred_conf).cpu(), pred_conf.shape)\n",
        "    score = pred_conf[gy, gx].item()\n",
        "\n",
        "    print(f\"[{idx}ë²ˆ ìƒ˜í”Œ] ê°ì§€ í™•ë¥ : {score*100:.2f}% (Grid: {gx}, {gy})\")\n",
        "\n",
        "    if score < conf_thresh:\n",
        "        print(\"âš ï¸ ê°ì§€ í™•ë¥ ì´ ë‚®ì•„ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    # 1) ì˜ˆì¸¡ ëœë“œë§ˆí¬ (Red)\n",
        "    kpt_vec = kpt_out[0, :, gy, gx]\n",
        "    kpt_vec = torch.sigmoid(kpt_vec).cpu().numpy()\n",
        "\n",
        "    pred_x = kpt_vec[0::3] * w\n",
        "    pred_y = kpt_vec[1::3] * h\n",
        "\n",
        "    # 2) ì •ë‹µ ëœë“œë§ˆí¬ (Green)\n",
        "    gt_kpts = target['landmarks']\n",
        "    gt_x = gt_kpts[:, 0].numpy() * w\n",
        "    gt_y = gt_kpts[:, 1].numpy() * h\n",
        "\n",
        "    # --- ê·¸ë¦¬ê¸° ---\n",
        "    vis_img = img_np.copy()\n",
        "\n",
        "    # ì •ë‹µ ê·¸ë¦¬ê¸° (ì´ˆë¡ìƒ‰)\n",
        "    for px, py in zip(gt_x, gt_y):\n",
        "        cv2.circle(vis_img, (int(px), int(py)), 3, (0, 255, 0), -1)\n",
        "\n",
        "    # ì˜ˆì¸¡ ê·¸ë¦¬ê¸° (ë¹¨ê°„ìƒ‰)\n",
        "    for px, py in zip(pred_x, pred_y):\n",
        "        cv2.circle(vis_img, (int(px), int(py)), 2, (0, 0, 255), -1)\n",
        "\n",
        "    # 3) ì •ë‹µ ë°•ìŠ¤ (ì´ˆë¡ìƒ‰) - ğŸ”§ [ìˆ˜ì •ëœ ë¶€ë¶„]\n",
        "    gt_bbox = target['bbox']\n",
        "\n",
        "    # .flatten()ì„ ì¶”ê°€í•˜ì—¬ (1, 4) -> (4,)ë¡œ í´ì¤ë‹ˆë‹¤.\n",
        "    bx, by, bw, bh = gt_bbox.flatten().numpy()\n",
        "\n",
        "    x1 = int((bx - bw/2) * w)\n",
        "    y1 = int((by - bh/2) * h)\n",
        "    x2 = int((bx + bw/2) * w)\n",
        "    y2 = int((by + bh/2) * h)\n",
        "    cv2.rectangle(vis_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.imshow(cv2.cvtColor(vis_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"Green: GT / Red: Pred (Conf: {score:.2f})\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# ë‹¤ì‹œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
        "print(\"ğŸ” 0ë²ˆ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸\")\n",
        "compare_prediction(model, train_dataset, idx=29)"
      ],
      "metadata": {
        "id": "bsQGFiHZ2iy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# í•™ìŠµ ì¢…ë£Œ í›„ ëª¨ë¸ ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ\n",
        "# ==========================================\n",
        "import torch\n",
        "\n",
        "# 1. íŒŒì¼ ì´ë¦„ ì •ì˜\n",
        "save_path = 'yolomt_final2.pt'\n",
        "\n",
        "# 2. ëª¨ë¸ ìƒíƒœ ì €ì¥ (ê°€ì¤‘ì¹˜ + ì˜µí‹°ë§ˆì´ì € ì •ë³´ í¬í•¨)\n",
        "torch.save({\n",
        "    'epoch': EPOCHS,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': epoch_loss,\n",
        "}, save_path)\n",
        "\n",
        "print(f\"âœ… ëª¨ë¸ì´ '{save_path}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# 3. ë‚´ ì»´í“¨í„°ë¡œ ë‹¤ìš´ë¡œë“œ (Google Colab ì „ìš©)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(save_path)\n",
        "    print(\"â¬‡ï¸ ë‹¤ìš´ë¡œë“œê°€ ì‹œì‘ë©ë‹ˆë‹¤...\")\n",
        "except ImportError:\n",
        "    print(\"âš ï¸ Google Colab í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤. íŒŒì¼ì´ ë¡œì»¬ ê²½ë¡œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "2aNG8Ahj88mt",
        "outputId": "3f176e4f-0589-4833-980b-a4b1972329bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ëª¨ë¸ì´ 'yolomt_final2.pt'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_38648b79-e541-4e1e-97b1-f94374656c33\", \"yolomt_final2.pt\", 14623214)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸ ë‹¤ìš´ë¡œë“œê°€ ì‹œì‘ë©ë‹ˆë‹¤...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import onnx\n",
        "from onnxsim import simplify\n",
        "\n",
        "# 1. ì„¤ì •\n",
        "IMG_SIZE = 320\n",
        "BATCH_SIZE = 1\n",
        "onnx_path = 'yolomt_best2.onnx'\n",
        "# ì €ì¥ëœ pt íŒŒì¼ ê²½ë¡œ í™•ì¸!\n",
        "ckpt_path = '/content/drive/yolomt_best2.pt'\n",
        "\n",
        "# 2. ëª¨ë¸ ì¬ì„±ì„± ë° ë¡œë“œ (ìˆ˜ì •ëœ í´ë˜ìŠ¤ ì ìš©ì„ ìœ„í•´ í•„ìˆ˜)\n",
        "print(\"â™»ï¸ Re-initializing model with fixed RepConv...\")\n",
        "model = SimplifiedYOLOMT(nc=1, n_keypoints=68).to(device)\n",
        "\n",
        "if os.path.exists(ckpt_path):\n",
        "    print(f\"ğŸ”„ Loading weights from {ckpt_path}...\")\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "else:\n",
        "    print(\"âš ï¸ Checkpoint file not found! Using random weights for test.\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# 3. Fuse (ì´ì œ ì—ëŸ¬ê°€ ì•ˆ ë‚  ê²ƒì…ë‹ˆë‹¤)\n",
        "print(\"ğŸ”§ Fusing RepConv layers...\")\n",
        "model.fuse()\n",
        "\n",
        "# 4. Export\n",
        "dummy_input = torch.randn(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
        "print(f\"ğŸš€ Exporting to ONNX...\")\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    onnx_path,\n",
        "    verbose=False,\n",
        "    opset_version=11,\n",
        "    input_names=['input'],\n",
        "    output_names=['det_out', 'kpt_out'],\n",
        "    do_constant_folding=True\n",
        ")\n",
        "print(f\"âœ… ONNX Exported: {onnx_path}\")\n",
        "\n",
        "# 5. Simplify\n",
        "try:\n",
        "    model_simp, check = simplify(onnx_path)\n",
        "    onnx.save(model_simp, onnx_path)\n",
        "    print(\"âœ… ONNX Simplified!\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Simplify failed: {e}\")\n",
        "\n",
        "# 6. Download\n",
        "from google.colab import files\n",
        "files.download(onnx_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "iFvlbdo8ASpG",
        "outputId": "a71154c5-9d35-42f3-ca05-1419b03841ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â™»ï¸ Re-initializing model with fixed RepConv...\n",
            "âš ï¸ Checkpoint file not found! Using random weights for test.\n",
            "ğŸ”§ Fusing RepConv layers...\n",
            "Fusing model layers for inference...\n",
            "ğŸš€ Exporting to ONNX...\n",
            "âœ… ONNX Exported: yolomt_best2.onnx\n",
            "âœ… ONNX Simplified!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_02e9e06b-41a8-43bb-8a46-034bef5782ef\", \"yolomt_best2.onnx\", 6592003)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8IWK2TzeDidK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}